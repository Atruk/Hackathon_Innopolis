{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Krypton_new_model_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atruk/Hackathon_Innopolis/blob/main/Krypton_new_model_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "UIZegx-v9D3B"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IqL0dno99T79",
        "outputId": "bbe4601b-72fd-4f3e-ace8-fb82769244a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.8.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dghARTqLCJXq"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "from tensorflow.keras.models import Model, Sequential, load_model \n",
        "import re \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
        "from tensorflow.keras import backend as K \n",
        "from tensorflow.keras.optimizers import Adam, Adamax, Adadelta \n",
        "# from tensorflow.keras import utils \n",
        "from google.colab import files \n",
        "import matplotlib.pyplot as plt \n",
        "from gensim.models import word2vec \n",
        "import os \n",
        "import pandas as pd \n",
        "import time \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from collections import Counter\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.preprocessing import FunctionTransformer\n",
        "# from sklearn.impute import SimpleImputer\n",
        "import os\n",
        "import time\n",
        "from functools import wraps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGADYDVa2mm3",
        "outputId": "018c898d-3917-4e1c-c1f3-0d32f2c1dacc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imblearn"
      ],
      "metadata": {
        "id": "wWzfgqBL19em"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imblearn.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6SWQL4AR2GX3",
        "outputId": "bc805a00-1a9f-4dfa-a076-c6f412deb9e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.8.1'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keras --upgrade"
      ],
      "metadata": {
        "id": "_tw5zsOO-Kfg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from imblearn.keras import BalancedBatchGenerator\n",
        "from imblearn.tensorflow import balanced_batch_generator"
      ],
      "metadata": {
        "id": "9x0mcCF7cVep"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_y_train = '/content/drive/MyDrive/Hackathon_Innopolis/train.csv'\n",
        "get_x_train = '/content/drive/MyDrive/Hackathon_Innopolis/new_train_dataset.csv'"
      ],
      "metadata": {
        "id": "cMZ-yG4zleC-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = pd.read_csv(get_y_train, usecols=['Active'])"
      ],
      "metadata": {
        "id": "f48ZcdlxleNS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[\"Active\"] = y_train[\"Active\"].astype(\"int64\")\n",
        "y_train.shape"
      ],
      "metadata": {
        "id": "lJ13JeXMleS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40491b0e-f99b-4509-85f3-f58ed2a7277c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5557, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg, pos = np.bincount(y_train['Active'])\n",
        "total = neg + pos\n",
        "total, neg, pos\n",
        "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, pos, 100 * pos / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPTD9dapYYjM",
        "outputId": "f580380c-2caa-4b94-d9cf-429c408c464c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples:\n",
            "    Total: 5557\n",
            "    Positive: 206 (3.71% of total)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = pd.read_csv(get_x_train)"
      ],
      "metadata": {
        "id": "6nffVMN5leWC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(get_x_train)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5zPre5bJNwg",
        "outputId": "0863e0a3-95fd-4ca5-aff7-8b66e2a88a6f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5557 entries, 0 to 5556\n",
            "Columns: 104 entries, tpsa to 99\n",
            "dtypes: float64(102), int64(2)\n",
            "memory usage: 4.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2RfCGDYFzkH",
        "outputId": "2fcc4b9c-cfdc-4cfd-cbb7-98405e27a4aa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5557, 104)\n",
            "(5557, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y = y_train['Active'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train,\n",
        "                                                    test_size=.20, shuffle=True, # .20, 0.30, 0.1\n",
        "                                                    stratify=y_train, random_state=123) # 123, 42"
      ],
      "metadata": {
        "id": "WKvvKjslpGji"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "X_test = StandardScaler().fit_transform(X_test)"
      ],
      "metadata": {
        "id": "5vho2tNQKJMS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NKNBeTZG952",
        "outputId": "771a36b8-3ac4-4282-9ae8-c05b7f1774dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4445, 104)\n",
            "(4445, 1)\n",
            "(1112, 104)\n",
            "(1112, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model_v1"
      ],
      "metadata": {
        "id": "RDtxIw8qauPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weights = '/content/drive/MyDrive/Hackathon_Innopolis/model_weights_v2.h5'\n",
        "model = '/content/drive/MyDrive/Hackathon_Innopolis/model_v2.h5'\n",
        "model = load_model(model)"
      ],
      "metadata": {
        "id": "IuhMPAuGpnBd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In one of the experiments on the selection of the architecture of the neural network, we obtained and saved such a model, where on the training data we got F1-score = 0.36"
      ],
      "metadata": {
        "id": "fV6R6346G_NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with f1_score = 0.36 in train data \n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJl9WyMG7ieJ",
        "outputId": "1a4629d2-5e1b-4c76-fa54-68f5cace609c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_61\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_298 (Dense)           (None, 256)               26880     \n",
            "                                                                 \n",
            " batch_normalization_83 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_150 (Dropout)       (None, 256)               0         \n",
            "                                                                 \n",
            " dense_299 (Dense)           (None, 224)               57568     \n",
            "                                                                 \n",
            " dropout_151 (Dropout)       (None, 224)               0         \n",
            "                                                                 \n",
            " dense_300 (Dense)           (None, 164)               36900     \n",
            "                                                                 \n",
            " dense_301 (Dense)           (None, 128)               21120     \n",
            "                                                                 \n",
            " dropout_152 (Dropout)       (None, 128)               0         \n",
            "                                                                 \n",
            " dense_302 (Dense)           (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_303 (Dense)           (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 151,813\n",
            "Trainable params: 151,301\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counts = np.bincount(y_train.values[:, 0])\n",
        "# print(\n",
        "#     \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
        "#         counts[1], 100 * float(counts[1]) / len(y_train.values)\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# weight_for_0 = 1.0 / counts[0]\n",
        "# weight_for_1 = 1.0 / counts[1]\n",
        "# class_weight = {0: weight_for_0, 1: weight_for_1}"
      ],
      "metadata": {
        "id": "-M7-asGkUtET"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# weight_for_0, weight_for_1"
      ],
      "metadata": {
        "id": "fSX4fH9GW1KZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "weight_for_0 = (1 / neg) * (total / 0.5) # 2.0\n",
        "weight_for_1 = (1 / pos) * (total / 3) # 2.0\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qfA_feSa4T9",
        "outputId": "5b09031b-a254-4eee-c5f2-39b7571a47f5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight for class 0: 2.08\n",
            "Weight for class 1: 8.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createModel():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, input_dim = X_train.shape[1], activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(164, activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(164, activation='relu'))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # model.load_weights('model.h5')\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.001))\n",
        "  return model"
      ],
      "metadata": {
        "id": "Fo7-fBt7H7lm"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = createModel()"
      ],
      "metadata": {
        "id": "DXUqTBGhIWJF"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = model.fit(X_train, y_train, batch_size = 128, epochs = 200, class_weight=class_weight)"
      ],
      "metadata": {
        "id": "jK1mjTy_QcoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473b51ec-7dd6-41dd-b4fb-95543e11f04d"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "35/35 [==============================] - 1s 4ms/step - loss: 1.0887\n",
            "Epoch 2/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.9376\n",
            "Epoch 3/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.9203\n",
            "Epoch 4/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.8623\n",
            "Epoch 5/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.8888\n",
            "Epoch 6/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.8072\n",
            "Epoch 7/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.7839\n",
            "Epoch 8/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.7506\n",
            "Epoch 9/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.7265\n",
            "Epoch 10/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.7233\n",
            "Epoch 11/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.6760\n",
            "Epoch 12/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.6561\n",
            "Epoch 13/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.6291\n",
            "Epoch 14/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.6396\n",
            "Epoch 15/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.5906\n",
            "Epoch 16/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.6135\n",
            "Epoch 17/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.5986\n",
            "Epoch 18/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.5701\n",
            "Epoch 19/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.5501\n",
            "Epoch 20/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.5633\n",
            "Epoch 21/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.5151\n",
            "Epoch 22/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.5098\n",
            "Epoch 23/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.4925\n",
            "Epoch 24/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.4747\n",
            "Epoch 25/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4879\n",
            "Epoch 26/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.4730\n",
            "Epoch 27/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4559\n",
            "Epoch 28/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.4455\n",
            "Epoch 29/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.4125\n",
            "Epoch 30/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3990\n",
            "Epoch 31/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.4687\n",
            "Epoch 32/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.4441\n",
            "Epoch 33/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3576\n",
            "Epoch 34/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3718\n",
            "Epoch 35/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3664\n",
            "Epoch 36/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3567\n",
            "Epoch 37/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3024\n",
            "Epoch 38/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3641\n",
            "Epoch 39/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3112\n",
            "Epoch 40/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3151\n",
            "Epoch 41/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3309\n",
            "Epoch 42/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.3123\n",
            "Epoch 43/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3161\n",
            "Epoch 44/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3043\n",
            "Epoch 45/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2833\n",
            "Epoch 46/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2533\n",
            "Epoch 47/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3337\n",
            "Epoch 48/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3064\n",
            "Epoch 49/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3078\n",
            "Epoch 50/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2413\n",
            "Epoch 51/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2879\n",
            "Epoch 52/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2749\n",
            "Epoch 53/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2374\n",
            "Epoch 54/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2502\n",
            "Epoch 55/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1924\n",
            "Epoch 56/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2268\n",
            "Epoch 57/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2442\n",
            "Epoch 58/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2243\n",
            "Epoch 59/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2035\n",
            "Epoch 60/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2393\n",
            "Epoch 61/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2324\n",
            "Epoch 62/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2285\n",
            "Epoch 63/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1950\n",
            "Epoch 64/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1855\n",
            "Epoch 65/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2037\n",
            "Epoch 66/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2498\n",
            "Epoch 67/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2710\n",
            "Epoch 68/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2306\n",
            "Epoch 69/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1839\n",
            "Epoch 70/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1667\n",
            "Epoch 71/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.2155\n",
            "Epoch 72/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1781\n",
            "Epoch 73/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1782\n",
            "Epoch 74/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1688\n",
            "Epoch 75/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1923\n",
            "Epoch 76/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1741\n",
            "Epoch 77/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1487\n",
            "Epoch 78/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2171\n",
            "Epoch 79/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1916\n",
            "Epoch 80/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1637\n",
            "Epoch 81/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2050\n",
            "Epoch 82/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2090\n",
            "Epoch 83/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1463\n",
            "Epoch 84/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1425\n",
            "Epoch 85/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1835\n",
            "Epoch 86/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1666\n",
            "Epoch 87/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1631\n",
            "Epoch 88/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1330\n",
            "Epoch 89/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1588\n",
            "Epoch 90/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1725\n",
            "Epoch 91/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1401\n",
            "Epoch 92/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1202\n",
            "Epoch 93/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1482\n",
            "Epoch 94/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1767\n",
            "Epoch 95/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1412\n",
            "Epoch 96/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1224\n",
            "Epoch 97/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1256\n",
            "Epoch 98/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1328\n",
            "Epoch 99/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1344\n",
            "Epoch 100/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1095\n",
            "Epoch 101/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1105\n",
            "Epoch 102/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1441\n",
            "Epoch 103/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1613\n",
            "Epoch 104/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1241\n",
            "Epoch 105/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1373\n",
            "Epoch 106/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2159\n",
            "Epoch 107/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1240\n",
            "Epoch 108/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1335\n",
            "Epoch 109/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1207\n",
            "Epoch 110/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1300\n",
            "Epoch 111/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1103\n",
            "Epoch 112/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1140\n",
            "Epoch 113/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1087\n",
            "Epoch 114/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1354\n",
            "Epoch 115/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0979\n",
            "Epoch 116/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1413\n",
            "Epoch 117/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1129\n",
            "Epoch 118/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1650\n",
            "Epoch 119/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1693\n",
            "Epoch 120/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1108\n",
            "Epoch 121/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1061\n",
            "Epoch 122/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0946\n",
            "Epoch 123/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1333\n",
            "Epoch 124/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1612\n",
            "Epoch 125/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1500\n",
            "Epoch 126/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1538\n",
            "Epoch 127/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1147\n",
            "Epoch 128/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0848\n",
            "Epoch 129/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1306\n",
            "Epoch 130/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.1374\n",
            "Epoch 131/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1335\n",
            "Epoch 132/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1743\n",
            "Epoch 133/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1038\n",
            "Epoch 134/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1088\n",
            "Epoch 135/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1403\n",
            "Epoch 136/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1204\n",
            "Epoch 137/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1321\n",
            "Epoch 138/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0970\n",
            "Epoch 139/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1107\n",
            "Epoch 140/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1493\n",
            "Epoch 141/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0908\n",
            "Epoch 142/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0988\n",
            "Epoch 143/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1082\n",
            "Epoch 144/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0748\n",
            "Epoch 145/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0961\n",
            "Epoch 146/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1027\n",
            "Epoch 147/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0907\n",
            "Epoch 148/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0703\n",
            "Epoch 149/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0879\n",
            "Epoch 150/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0843\n",
            "Epoch 151/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1087\n",
            "Epoch 152/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1100\n",
            "Epoch 153/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1050\n",
            "Epoch 154/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1327\n",
            "Epoch 155/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0923\n",
            "Epoch 156/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1121\n",
            "Epoch 157/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1078\n",
            "Epoch 158/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1262\n",
            "Epoch 159/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1032\n",
            "Epoch 160/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0998\n",
            "Epoch 161/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0889\n",
            "Epoch 162/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1201\n",
            "Epoch 163/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1115\n",
            "Epoch 164/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0741\n",
            "Epoch 165/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1017\n",
            "Epoch 166/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0960\n",
            "Epoch 167/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1146\n",
            "Epoch 168/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1183\n",
            "Epoch 169/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0892\n",
            "Epoch 170/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0894\n",
            "Epoch 171/200\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0905\n",
            "Epoch 172/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0936\n",
            "Epoch 173/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0713\n",
            "Epoch 174/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1303\n",
            "Epoch 175/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1217\n",
            "Epoch 176/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0924\n",
            "Epoch 177/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1165\n",
            "Epoch 178/200\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0930\n",
            "Epoch 179/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1008\n",
            "Epoch 180/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0906\n",
            "Epoch 181/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0832\n",
            "Epoch 182/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0795\n",
            "Epoch 183/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1431\n",
            "Epoch 184/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0879\n",
            "Epoch 185/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0723\n",
            "Epoch 186/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1052\n",
            "Epoch 187/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1147\n",
            "Epoch 188/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0963\n",
            "Epoch 189/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0835\n",
            "Epoch 190/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0793\n",
            "Epoch 191/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0568\n",
            "Epoch 192/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0661\n",
            "Epoch 193/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0599\n",
            "Epoch 194/200\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0671\n",
            "Epoch 195/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1002\n",
            "Epoch 196/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0422\n",
            "Epoch 197/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0687\n",
            "Epoch 198/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0584\n",
            "Epoch 199/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0779\n",
            "Epoch 200/200\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train.history['loss'], label='loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "OJCA-WD3LUmF",
        "outputId": "0d7996a7-f89f-4c69-8cbd-2a50748bbab9"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = (model.predict(X_test) > 0.5).astype(\"int\")\n",
        "values, counts = np.unique(pred, return_counts=True)\n",
        "print(values, counts)\n",
        "f1_score(y_test, pred, average='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kja3nwRsL_oV",
        "outputId": "f8ef2681-4b1c-4c99-a3b5-5b6516b2d74b"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1] [1070   42]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21686746987951805"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test, pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c8mlEQHgn4Z",
        "outputId": "7a092160-e0f5-46ed-da7a-714ed9b585b1"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1038,   33],\n",
              "       [  32,    9]])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "values, counts = np.unique(y_test, return_counts=True)\n",
        "print(values, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZH-5bqJf5NC",
        "outputId": "1cb437d3-eec2-4659-deb0-737c7d8d80db"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1] [1071   41]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save_weights('model_weights_v2.h5')\n",
        "# model.save('model_v2.h5')"
      ],
      "metadata": {
        "id": "mMje1ZGnQHIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model_v2"
      ],
      "metadata": {
        "id": "5EY-WhOda79V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    BatchNormalization,\n",
        ")\n",
        "\n",
        "\n",
        "def make_model(n_features):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(200, input_shape=(n_features,), kernel_initializer=\"glorot_normal\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, kernel_initializer=\"glorot_normal\", use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(50, kernel_initializer=\"glorot_normal\", use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.15))\n",
        "    model.add(Dense(25, kernel_initializer=\"glorot_normal\", use_bias=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9DoeK0KLa-v0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def timeit(f):\n",
        "#     @wraps(f)\n",
        "#     def wrapper(*args, **kwds):\n",
        "#         start_time = time.time()\n",
        "#         result = f(*args, **kwds)\n",
        "#         elapsed_time = time.time() - start_time\n",
        "#         print(f\"Elapsed computation time: {elapsed_time:.3f} secs\")\n",
        "#         return (elapsed_time, result)\n",
        "\n",
        "#     return wrapper"
      ],
      "metadata": {
        "id": "kKr9yoVT3Cit"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @ timeit\n",
        "# def fit_predict_imbalanced_model(X_train, y_train, X_test, y_test):\n",
        "#   model = make_model(X_train.shape[1])\n",
        "#   model.fit(X_train, y_train, epochs=2, verbose=1, batch_size=128)\n",
        "#   y_pred = (model.predict(X_test) > 0.5).astype(\"int\") # model.predict_proba(X_test, batch_size=1000)\n",
        "#   return f1_score(y_test, y_pred) #roc_auc_score(y_test, y_pred),"
      ],
      "metadata": {
        "id": "XztacK6N3DR4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_model(X_train.shape[1])\n",
        "model.fit(X_train, y_train, epochs=500, verbose=1, batch_size=256, class_weight=class_weight)\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int\") \n",
        "f1_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ9cL2ucGC-6",
        "outputId": "56e5e71c-2d64-4526-86ff-5d11f510e915"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "18/18 [==============================] - 4s 6ms/step - loss: 1.8764 - accuracy: 0.4029\n",
            "Epoch 2/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.5478 - accuracy: 0.6517\n",
            "Epoch 3/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.3320 - accuracy: 0.8225\n",
            "Epoch 4/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.2004 - accuracy: 0.9010\n",
            "Epoch 5/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1007 - accuracy: 0.9339\n",
            "Epoch 6/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0429 - accuracy: 0.9424\n",
            "Epoch 7/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.9925 - accuracy: 0.9471\n",
            "Epoch 8/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9817 - accuracy: 0.9521\n",
            "Epoch 9/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9321 - accuracy: 0.9528\n",
            "Epoch 10/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9115 - accuracy: 0.9519\n",
            "Epoch 11/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8836 - accuracy: 0.9496\n",
            "Epoch 12/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8859 - accuracy: 0.9487\n",
            "Epoch 13/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8652 - accuracy: 0.9564\n",
            "Epoch 14/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8568 - accuracy: 0.9505\n",
            "Epoch 15/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8300 - accuracy: 0.9564\n",
            "Epoch 16/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8223 - accuracy: 0.9503\n",
            "Epoch 17/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.8226 - accuracy: 0.9510\n",
            "Epoch 18/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7872 - accuracy: 0.9539\n",
            "Epoch 19/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8131 - accuracy: 0.9510\n",
            "Epoch 20/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8083 - accuracy: 0.9485\n",
            "Epoch 21/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7977 - accuracy: 0.9501\n",
            "Epoch 22/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7705 - accuracy: 0.9532\n",
            "Epoch 23/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7779 - accuracy: 0.9512\n",
            "Epoch 24/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.7830 - accuracy: 0.9532\n",
            "Epoch 25/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7431 - accuracy: 0.9494\n",
            "Epoch 26/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7560 - accuracy: 0.9496\n",
            "Epoch 27/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7358 - accuracy: 0.9462\n",
            "Epoch 28/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7519 - accuracy: 0.9492\n",
            "Epoch 29/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7035 - accuracy: 0.9485\n",
            "Epoch 30/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7468 - accuracy: 0.9465\n",
            "Epoch 31/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7053 - accuracy: 0.9485\n",
            "Epoch 32/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6808 - accuracy: 0.9507\n",
            "Epoch 33/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7166 - accuracy: 0.9498\n",
            "Epoch 34/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.7206 - accuracy: 0.9496\n",
            "Epoch 35/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6774 - accuracy: 0.9503\n",
            "Epoch 36/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6829 - accuracy: 0.9442\n",
            "Epoch 37/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6707 - accuracy: 0.9467\n",
            "Epoch 38/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6962 - accuracy: 0.9485\n",
            "Epoch 39/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6674 - accuracy: 0.9514\n",
            "Epoch 40/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6730 - accuracy: 0.9483\n",
            "Epoch 41/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6596 - accuracy: 0.9447\n",
            "Epoch 42/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6285 - accuracy: 0.9478\n",
            "Epoch 43/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6512 - accuracy: 0.9444\n",
            "Epoch 44/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6362 - accuracy: 0.9534\n",
            "Epoch 45/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6590 - accuracy: 0.9456\n",
            "Epoch 46/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.6412 - accuracy: 0.9501\n",
            "Epoch 47/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.6216 - accuracy: 0.9550\n",
            "Epoch 48/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6123 - accuracy: 0.9570\n",
            "Epoch 49/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6021 - accuracy: 0.9566\n",
            "Epoch 50/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.6179 - accuracy: 0.9510\n",
            "Epoch 51/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6081 - accuracy: 0.9512\n",
            "Epoch 52/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6076 - accuracy: 0.9465\n",
            "Epoch 53/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6013 - accuracy: 0.9514\n",
            "Epoch 54/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5802 - accuracy: 0.9512\n",
            "Epoch 55/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5598 - accuracy: 0.9476\n",
            "Epoch 56/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5640 - accuracy: 0.9532\n",
            "Epoch 57/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5657 - accuracy: 0.9496\n",
            "Epoch 58/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5620 - accuracy: 0.9512\n",
            "Epoch 59/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5652 - accuracy: 0.9467\n",
            "Epoch 60/500\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.5428 - accuracy: 0.9537\n",
            "Epoch 61/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5790 - accuracy: 0.9503\n",
            "Epoch 62/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5756 - accuracy: 0.9480\n",
            "Epoch 63/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5557 - accuracy: 0.9478\n",
            "Epoch 64/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5335 - accuracy: 0.9557\n",
            "Epoch 65/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5379 - accuracy: 0.9514\n",
            "Epoch 66/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.5307 - accuracy: 0.9555\n",
            "Epoch 67/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5023 - accuracy: 0.9557\n",
            "Epoch 68/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5339 - accuracy: 0.9557\n",
            "Epoch 69/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5392 - accuracy: 0.9528\n",
            "Epoch 70/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5344 - accuracy: 0.9521\n",
            "Epoch 71/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5385 - accuracy: 0.9485\n",
            "Epoch 72/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5169 - accuracy: 0.9575\n",
            "Epoch 73/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4746 - accuracy: 0.9557\n",
            "Epoch 74/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5298 - accuracy: 0.9489\n",
            "Epoch 75/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.5017 - accuracy: 0.9528\n",
            "Epoch 76/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4737 - accuracy: 0.9546\n",
            "Epoch 77/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4812 - accuracy: 0.9557\n",
            "Epoch 78/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.5087 - accuracy: 0.9579\n",
            "Epoch 79/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4642 - accuracy: 0.9577\n",
            "Epoch 80/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4922 - accuracy: 0.9505\n",
            "Epoch 81/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4549 - accuracy: 0.9606\n",
            "Epoch 82/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4743 - accuracy: 0.9564\n",
            "Epoch 83/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4909 - accuracy: 0.9521\n",
            "Epoch 84/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4846 - accuracy: 0.9528\n",
            "Epoch 85/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4585 - accuracy: 0.9600\n",
            "Epoch 86/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4561 - accuracy: 0.9566\n",
            "Epoch 87/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.4232 - accuracy: 0.9588\n",
            "Epoch 88/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4339 - accuracy: 0.9541\n",
            "Epoch 89/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4625 - accuracy: 0.9584\n",
            "Epoch 90/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4639 - accuracy: 0.9548\n",
            "Epoch 91/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4657 - accuracy: 0.9561\n",
            "Epoch 92/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4170 - accuracy: 0.9550\n",
            "Epoch 93/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4797 - accuracy: 0.9597\n",
            "Epoch 94/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4049 - accuracy: 0.9588\n",
            "Epoch 95/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4332 - accuracy: 0.9541\n",
            "Epoch 96/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.4200 - accuracy: 0.9564\n",
            "Epoch 97/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4174 - accuracy: 0.9620\n",
            "Epoch 98/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4186 - accuracy: 0.9629\n",
            "Epoch 99/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4451 - accuracy: 0.9532\n",
            "Epoch 100/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4191 - accuracy: 0.9566\n",
            "Epoch 101/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3808 - accuracy: 0.9613\n",
            "Epoch 102/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3753 - accuracy: 0.9645\n",
            "Epoch 103/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3715 - accuracy: 0.9611\n",
            "Epoch 104/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4253 - accuracy: 0.9534\n",
            "Epoch 105/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4283 - accuracy: 0.9575\n",
            "Epoch 106/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3996 - accuracy: 0.9627\n",
            "Epoch 107/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3793 - accuracy: 0.9622\n",
            "Epoch 108/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.4392 - accuracy: 0.9537\n",
            "Epoch 109/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4048 - accuracy: 0.9566\n",
            "Epoch 110/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3793 - accuracy: 0.9604\n",
            "Epoch 111/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.4037 - accuracy: 0.9591\n",
            "Epoch 112/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3507 - accuracy: 0.9627\n",
            "Epoch 113/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3715 - accuracy: 0.9665\n",
            "Epoch 114/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3473 - accuracy: 0.9593\n",
            "Epoch 115/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3990 - accuracy: 0.9633\n",
            "Epoch 116/500\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.3849 - accuracy: 0.9586\n",
            "Epoch 117/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3803 - accuracy: 0.9606\n",
            "Epoch 118/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3537 - accuracy: 0.9600\n",
            "Epoch 119/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3779 - accuracy: 0.9629\n",
            "Epoch 120/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3332 - accuracy: 0.9633\n",
            "Epoch 121/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3156 - accuracy: 0.9615\n",
            "Epoch 122/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3673 - accuracy: 0.9624\n",
            "Epoch 123/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3492 - accuracy: 0.9624\n",
            "Epoch 124/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3748 - accuracy: 0.9548\n",
            "Epoch 125/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3314 - accuracy: 0.9629\n",
            "Epoch 126/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3358 - accuracy: 0.9703\n",
            "Epoch 127/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3316 - accuracy: 0.9683\n",
            "Epoch 128/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3407 - accuracy: 0.9624\n",
            "Epoch 129/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3371 - accuracy: 0.9570\n",
            "Epoch 130/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3581 - accuracy: 0.9654\n",
            "Epoch 131/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3572 - accuracy: 0.9543\n",
            "Epoch 132/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3683 - accuracy: 0.9636\n",
            "Epoch 133/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3706 - accuracy: 0.9586\n",
            "Epoch 134/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3355 - accuracy: 0.9618\n",
            "Epoch 135/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3208 - accuracy: 0.9660\n",
            "Epoch 136/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.2929 - accuracy: 0.9681\n",
            "Epoch 137/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3144 - accuracy: 0.9642\n",
            "Epoch 138/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3053 - accuracy: 0.9627\n",
            "Epoch 139/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3138 - accuracy: 0.9681\n",
            "Epoch 140/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3025 - accuracy: 0.9681\n",
            "Epoch 141/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3115 - accuracy: 0.9651\n",
            "Epoch 142/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3048 - accuracy: 0.9645\n",
            "Epoch 143/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2943 - accuracy: 0.9681\n",
            "Epoch 144/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3082 - accuracy: 0.9651\n",
            "Epoch 145/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3153 - accuracy: 0.9613\n",
            "Epoch 146/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3282 - accuracy: 0.9613\n",
            "Epoch 147/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3234 - accuracy: 0.9651\n",
            "Epoch 148/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3317 - accuracy: 0.9615\n",
            "Epoch 149/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3113 - accuracy: 0.9613\n",
            "Epoch 150/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3154 - accuracy: 0.9651\n",
            "Epoch 151/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.2848 - accuracy: 0.9678\n",
            "Epoch 152/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3152 - accuracy: 0.9636\n",
            "Epoch 153/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3144 - accuracy: 0.9696\n",
            "Epoch 154/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3219 - accuracy: 0.9667\n",
            "Epoch 155/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3105 - accuracy: 0.9681\n",
            "Epoch 156/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2996 - accuracy: 0.9667\n",
            "Epoch 157/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.3108 - accuracy: 0.9665\n",
            "Epoch 158/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2796 - accuracy: 0.9696\n",
            "Epoch 159/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.3336 - accuracy: 0.9651\n",
            "Epoch 160/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2884 - accuracy: 0.9699\n",
            "Epoch 161/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.3204 - accuracy: 0.9665\n",
            "Epoch 162/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2457 - accuracy: 0.9723\n",
            "Epoch 163/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2709 - accuracy: 0.9705\n",
            "Epoch 164/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.2643 - accuracy: 0.9692\n",
            "Epoch 165/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.2661 - accuracy: 0.9777\n",
            "Epoch 166/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2580 - accuracy: 0.9669\n",
            "Epoch 167/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2779 - accuracy: 0.9687\n",
            "Epoch 168/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.3009 - accuracy: 0.9696\n",
            "Epoch 169/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2591 - accuracy: 0.9714\n",
            "Epoch 170/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2832 - accuracy: 0.9672\n",
            "Epoch 171/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2790 - accuracy: 0.9631\n",
            "Epoch 172/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2539 - accuracy: 0.9721\n",
            "Epoch 173/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2854 - accuracy: 0.9744\n",
            "Epoch 174/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.3163 - accuracy: 0.9627\n",
            "Epoch 175/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.3016 - accuracy: 0.9672\n",
            "Epoch 176/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2640 - accuracy: 0.9656\n",
            "Epoch 177/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2723 - accuracy: 0.9708\n",
            "Epoch 178/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2607 - accuracy: 0.9705\n",
            "Epoch 179/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2595 - accuracy: 0.9676\n",
            "Epoch 180/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2714 - accuracy: 0.9732\n",
            "Epoch 181/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2436 - accuracy: 0.9710\n",
            "Epoch 182/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2668 - accuracy: 0.9683\n",
            "Epoch 183/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2745 - accuracy: 0.9667\n",
            "Epoch 184/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.2519 - accuracy: 0.9721\n",
            "Epoch 185/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2678 - accuracy: 0.9710\n",
            "Epoch 186/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2473 - accuracy: 0.9717\n",
            "Epoch 187/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2223 - accuracy: 0.9759\n",
            "Epoch 188/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2352 - accuracy: 0.9753\n",
            "Epoch 189/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.2133 - accuracy: 0.9759\n",
            "Epoch 190/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2463 - accuracy: 0.9741\n",
            "Epoch 191/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2295 - accuracy: 0.9694\n",
            "Epoch 192/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2214 - accuracy: 0.9766\n",
            "Epoch 193/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2212 - accuracy: 0.9737\n",
            "Epoch 194/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2351 - accuracy: 0.9768\n",
            "Epoch 195/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2385 - accuracy: 0.9732\n",
            "Epoch 196/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2013 - accuracy: 0.9755\n",
            "Epoch 197/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2498 - accuracy: 0.9732\n",
            "Epoch 198/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2433 - accuracy: 0.9719\n",
            "Epoch 199/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2418 - accuracy: 0.9726\n",
            "Epoch 200/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2155 - accuracy: 0.9764\n",
            "Epoch 201/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2380 - accuracy: 0.9741\n",
            "Epoch 202/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2583 - accuracy: 0.9705\n",
            "Epoch 203/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2082 - accuracy: 0.9748\n",
            "Epoch 204/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2064 - accuracy: 0.9773\n",
            "Epoch 205/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.1993 - accuracy: 0.9771\n",
            "Epoch 206/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.2117 - accuracy: 0.9780\n",
            "Epoch 207/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.1960 - accuracy: 0.9782\n",
            "Epoch 208/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2196 - accuracy: 0.9744\n",
            "Epoch 209/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.1732 - accuracy: 0.9798\n",
            "Epoch 210/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2379 - accuracy: 0.9735\n",
            "Epoch 211/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2196 - accuracy: 0.9744\n",
            "Epoch 212/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.2024 - accuracy: 0.9762\n",
            "Epoch 213/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2512 - accuracy: 0.9721\n",
            "Epoch 214/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2553 - accuracy: 0.9690\n",
            "Epoch 215/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2434 - accuracy: 0.9762\n",
            "Epoch 216/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2339 - accuracy: 0.9710\n",
            "Epoch 217/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2195 - accuracy: 0.9746\n",
            "Epoch 218/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2238 - accuracy: 0.9726\n",
            "Epoch 219/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.1923 - accuracy: 0.9755\n",
            "Epoch 220/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2288 - accuracy: 0.9771\n",
            "Epoch 221/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.2154 - accuracy: 0.9728\n",
            "Epoch 222/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2135 - accuracy: 0.9741\n",
            "Epoch 223/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.2026 - accuracy: 0.9748\n",
            "Epoch 224/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.1965 - accuracy: 0.9771\n",
            "Epoch 225/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.2380 - accuracy: 0.9773\n",
            "Epoch 226/500\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.1857 - accuracy: 0.9764\n",
            "Epoch 227/500\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.2097 - accuracy: 0.9771\n",
            "Epoch 228/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.1860 - accuracy: 0.9793\n",
            "Epoch 229/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2196 - accuracy: 0.9771\n",
            "Epoch 230/500\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.2602 - accuracy: 0.9708\n",
            "Epoch 231/500\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.1964 - accuracy: 0.9775\n",
            "Epoch 232/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2525 - accuracy: 0.9723\n",
            "Epoch 233/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.2059 - accuracy: 0.9748\n",
            "Epoch 234/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1794 - accuracy: 0.9804\n",
            "Epoch 235/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2134 - accuracy: 0.9744\n",
            "Epoch 236/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1955 - accuracy: 0.9780\n",
            "Epoch 237/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2023 - accuracy: 0.9773\n",
            "Epoch 238/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1934 - accuracy: 0.9809\n",
            "Epoch 239/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1722 - accuracy: 0.9811\n",
            "Epoch 240/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1759 - accuracy: 0.9800\n",
            "Epoch 241/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2080 - accuracy: 0.9782\n",
            "Epoch 242/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1827 - accuracy: 0.9784\n",
            "Epoch 243/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1968 - accuracy: 0.9753\n",
            "Epoch 244/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2203 - accuracy: 0.9710\n",
            "Epoch 245/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1907 - accuracy: 0.9813\n",
            "Epoch 246/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1875 - accuracy: 0.9764\n",
            "Epoch 247/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2026 - accuracy: 0.9773\n",
            "Epoch 248/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2134 - accuracy: 0.9748\n",
            "Epoch 249/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1857 - accuracy: 0.9791\n",
            "Epoch 250/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1960 - accuracy: 0.9791\n",
            "Epoch 251/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2101 - accuracy: 0.9708\n",
            "Epoch 252/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1882 - accuracy: 0.9811\n",
            "Epoch 253/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1665 - accuracy: 0.9798\n",
            "Epoch 254/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1922 - accuracy: 0.9748\n",
            "Epoch 255/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1839 - accuracy: 0.9802\n",
            "Epoch 256/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1959 - accuracy: 0.9746\n",
            "Epoch 257/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2136 - accuracy: 0.9775\n",
            "Epoch 258/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.2057 - accuracy: 0.9764\n",
            "Epoch 259/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2443 - accuracy: 0.9759\n",
            "Epoch 260/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1719 - accuracy: 0.9793\n",
            "Epoch 261/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1852 - accuracy: 0.9768\n",
            "Epoch 262/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1791 - accuracy: 0.9811\n",
            "Epoch 263/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1834 - accuracy: 0.9768\n",
            "Epoch 264/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1829 - accuracy: 0.9773\n",
            "Epoch 265/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1806 - accuracy: 0.9791\n",
            "Epoch 266/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2085 - accuracy: 0.9780\n",
            "Epoch 267/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1615 - accuracy: 0.9784\n",
            "Epoch 268/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2250 - accuracy: 0.9789\n",
            "Epoch 269/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1869 - accuracy: 0.9777\n",
            "Epoch 270/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1861 - accuracy: 0.9793\n",
            "Epoch 271/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.2143 - accuracy: 0.9735\n",
            "Epoch 272/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1888 - accuracy: 0.9791\n",
            "Epoch 273/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1864 - accuracy: 0.9795\n",
            "Epoch 274/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1645 - accuracy: 0.9845\n",
            "Epoch 275/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.2014 - accuracy: 0.9766\n",
            "Epoch 276/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1916 - accuracy: 0.9804\n",
            "Epoch 277/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1957 - accuracy: 0.9780\n",
            "Epoch 278/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1477 - accuracy: 0.9831\n",
            "Epoch 279/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1781 - accuracy: 0.9831\n",
            "Epoch 280/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1519 - accuracy: 0.9827\n",
            "Epoch 281/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1725 - accuracy: 0.9811\n",
            "Epoch 282/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1935 - accuracy: 0.9789\n",
            "Epoch 283/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1850 - accuracy: 0.9768\n",
            "Epoch 284/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1362 - accuracy: 0.9847\n",
            "Epoch 285/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1644 - accuracy: 0.9795\n",
            "Epoch 286/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1552 - accuracy: 0.9827\n",
            "Epoch 287/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1547 - accuracy: 0.9845\n",
            "Epoch 288/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1571 - accuracy: 0.9807\n",
            "Epoch 289/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1639 - accuracy: 0.9784\n",
            "Epoch 290/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1979 - accuracy: 0.9791\n",
            "Epoch 291/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1939 - accuracy: 0.9825\n",
            "Epoch 292/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1666 - accuracy: 0.9807\n",
            "Epoch 293/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1605 - accuracy: 0.9883\n",
            "Epoch 294/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1530 - accuracy: 0.9811\n",
            "Epoch 295/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1807 - accuracy: 0.9771\n",
            "Epoch 296/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1451 - accuracy: 0.9836\n",
            "Epoch 297/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1472 - accuracy: 0.9829\n",
            "Epoch 298/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1598 - accuracy: 0.9816\n",
            "Epoch 299/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1635 - accuracy: 0.9800\n",
            "Epoch 300/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1581 - accuracy: 0.9854\n",
            "Epoch 301/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1727 - accuracy: 0.9771\n",
            "Epoch 302/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1855 - accuracy: 0.9780\n",
            "Epoch 303/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1775 - accuracy: 0.9789\n",
            "Epoch 304/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1534 - accuracy: 0.9759\n",
            "Epoch 305/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1536 - accuracy: 0.9834\n",
            "Epoch 306/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1812 - accuracy: 0.9757\n",
            "Epoch 307/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1755 - accuracy: 0.9780\n",
            "Epoch 308/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1958 - accuracy: 0.9762\n",
            "Epoch 309/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1894 - accuracy: 0.9811\n",
            "Epoch 310/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1732 - accuracy: 0.9757\n",
            "Epoch 311/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1508 - accuracy: 0.9802\n",
            "Epoch 312/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1801 - accuracy: 0.9793\n",
            "Epoch 313/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1449 - accuracy: 0.9825\n",
            "Epoch 314/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1869 - accuracy: 0.9804\n",
            "Epoch 315/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1627 - accuracy: 0.9802\n",
            "Epoch 316/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1323 - accuracy: 0.9843\n",
            "Epoch 317/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1642 - accuracy: 0.9836\n",
            "Epoch 318/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1320 - accuracy: 0.9822\n",
            "Epoch 319/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1668 - accuracy: 0.9838\n",
            "Epoch 320/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1788 - accuracy: 0.9843\n",
            "Epoch 321/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1775 - accuracy: 0.9813\n",
            "Epoch 322/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1563 - accuracy: 0.9813\n",
            "Epoch 323/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1270 - accuracy: 0.9840\n",
            "Epoch 324/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1314 - accuracy: 0.9861\n",
            "Epoch 325/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1080 - accuracy: 0.9874\n",
            "Epoch 326/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1750 - accuracy: 0.9811\n",
            "Epoch 327/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1127 - accuracy: 0.9852\n",
            "Epoch 328/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1429 - accuracy: 0.9863\n",
            "Epoch 329/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1684 - accuracy: 0.9816\n",
            "Epoch 330/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1718 - accuracy: 0.9768\n",
            "Epoch 331/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1427 - accuracy: 0.9818\n",
            "Epoch 332/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1158 - accuracy: 0.9876\n",
            "Epoch 333/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1384 - accuracy: 0.9798\n",
            "Epoch 334/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1958 - accuracy: 0.9807\n",
            "Epoch 335/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1232 - accuracy: 0.9845\n",
            "Epoch 336/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1622 - accuracy: 0.9843\n",
            "Epoch 337/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1461 - accuracy: 0.9831\n",
            "Epoch 338/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1648 - accuracy: 0.9780\n",
            "Epoch 339/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1553 - accuracy: 0.9795\n",
            "Epoch 340/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1550 - accuracy: 0.9825\n",
            "Epoch 341/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1259 - accuracy: 0.9834\n",
            "Epoch 342/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1264 - accuracy: 0.9849\n",
            "Epoch 343/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1290 - accuracy: 0.9854\n",
            "Epoch 344/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1334 - accuracy: 0.9834\n",
            "Epoch 345/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1423 - accuracy: 0.9834\n",
            "Epoch 346/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1373 - accuracy: 0.9845\n",
            "Epoch 347/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1577 - accuracy: 0.9807\n",
            "Epoch 348/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1495 - accuracy: 0.9840\n",
            "Epoch 349/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1514 - accuracy: 0.9822\n",
            "Epoch 350/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1344 - accuracy: 0.9858\n",
            "Epoch 351/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1764 - accuracy: 0.9798\n",
            "Epoch 352/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1614 - accuracy: 0.9786\n",
            "Epoch 353/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1555 - accuracy: 0.9856\n",
            "Epoch 354/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.9795\n",
            "Epoch 355/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1310 - accuracy: 0.9863\n",
            "Epoch 356/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1495 - accuracy: 0.9838\n",
            "Epoch 357/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1513 - accuracy: 0.9816\n",
            "Epoch 358/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1342 - accuracy: 0.9858\n",
            "Epoch 359/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1395 - accuracy: 0.9834\n",
            "Epoch 360/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1183 - accuracy: 0.9885\n",
            "Epoch 361/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1193 - accuracy: 0.9843\n",
            "Epoch 362/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1051 - accuracy: 0.9879\n",
            "Epoch 363/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1218 - accuracy: 0.9847\n",
            "Epoch 364/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1154 - accuracy: 0.9836\n",
            "Epoch 365/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1437 - accuracy: 0.9867\n",
            "Epoch 366/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1250 - accuracy: 0.9867\n",
            "Epoch 367/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1623 - accuracy: 0.9811\n",
            "Epoch 368/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1204 - accuracy: 0.9874\n",
            "Epoch 369/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1552 - accuracy: 0.9827\n",
            "Epoch 370/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1344 - accuracy: 0.9840\n",
            "Epoch 371/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1163 - accuracy: 0.9876\n",
            "Epoch 372/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1126 - accuracy: 0.9874\n",
            "Epoch 373/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1352 - accuracy: 0.9856\n",
            "Epoch 374/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1356 - accuracy: 0.9852\n",
            "Epoch 375/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1269 - accuracy: 0.9861\n",
            "Epoch 376/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1344 - accuracy: 0.9867\n",
            "Epoch 377/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1155 - accuracy: 0.9847\n",
            "Epoch 378/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1010 - accuracy: 0.9858\n",
            "Epoch 379/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1208 - accuracy: 0.9881\n",
            "Epoch 380/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1441 - accuracy: 0.9831\n",
            "Epoch 381/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1554 - accuracy: 0.9831\n",
            "Epoch 382/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1293 - accuracy: 0.9822\n",
            "Epoch 383/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1312 - accuracy: 0.9836\n",
            "Epoch 384/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1281 - accuracy: 0.9881\n",
            "Epoch 385/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1429 - accuracy: 0.9849\n",
            "Epoch 386/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1265 - accuracy: 0.9854\n",
            "Epoch 387/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1547 - accuracy: 0.9863\n",
            "Epoch 388/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1400 - accuracy: 0.9847\n",
            "Epoch 389/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1319 - accuracy: 0.9865\n",
            "Epoch 390/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1138 - accuracy: 0.9879\n",
            "Epoch 391/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1597 - accuracy: 0.9840\n",
            "Epoch 392/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1265 - accuracy: 0.9845\n",
            "Epoch 393/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1082 - accuracy: 0.9865\n",
            "Epoch 394/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1332 - accuracy: 0.9847\n",
            "Epoch 395/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1337 - accuracy: 0.9834\n",
            "Epoch 396/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1275 - accuracy: 0.9858\n",
            "Epoch 397/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1355 - accuracy: 0.9858\n",
            "Epoch 398/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1197 - accuracy: 0.9858\n",
            "Epoch 399/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1236 - accuracy: 0.9843\n",
            "Epoch 400/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1233 - accuracy: 0.9838\n",
            "Epoch 401/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1290 - accuracy: 0.9858\n",
            "Epoch 402/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1138 - accuracy: 0.9856\n",
            "Epoch 403/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1142 - accuracy: 0.9876\n",
            "Epoch 404/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1042 - accuracy: 0.9888\n",
            "Epoch 405/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1183 - accuracy: 0.9872\n",
            "Epoch 406/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1444 - accuracy: 0.9840\n",
            "Epoch 407/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1303 - accuracy: 0.9854\n",
            "Epoch 408/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1329 - accuracy: 0.9849\n",
            "Epoch 409/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1596 - accuracy: 0.9847\n",
            "Epoch 410/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1212 - accuracy: 0.9861\n",
            "Epoch 411/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1099 - accuracy: 0.9870\n",
            "Epoch 412/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0887 - accuracy: 0.9899\n",
            "Epoch 413/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1204 - accuracy: 0.9822\n",
            "Epoch 414/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1533 - accuracy: 0.9827\n",
            "Epoch 415/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1596 - accuracy: 0.9809\n",
            "Epoch 416/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1185 - accuracy: 0.9845\n",
            "Epoch 417/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1297 - accuracy: 0.9847\n",
            "Epoch 418/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1089 - accuracy: 0.9892\n",
            "Epoch 419/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1366 - accuracy: 0.9872\n",
            "Epoch 420/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1277 - accuracy: 0.9861\n",
            "Epoch 421/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1126 - accuracy: 0.9840\n",
            "Epoch 422/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1294 - accuracy: 0.9872\n",
            "Epoch 423/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9852\n",
            "Epoch 424/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1200 - accuracy: 0.9861\n",
            "Epoch 425/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1025 - accuracy: 0.9867\n",
            "Epoch 426/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1296 - accuracy: 0.9872\n",
            "Epoch 427/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1056 - accuracy: 0.9867\n",
            "Epoch 428/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0933 - accuracy: 0.9885\n",
            "Epoch 429/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1245 - accuracy: 0.9867\n",
            "Epoch 430/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1161 - accuracy: 0.9861\n",
            "Epoch 431/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1487 - accuracy: 0.9847\n",
            "Epoch 432/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1488 - accuracy: 0.9793\n",
            "Epoch 433/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1075 - accuracy: 0.9879\n",
            "Epoch 434/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1197 - accuracy: 0.9847\n",
            "Epoch 435/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1289 - accuracy: 0.9863\n",
            "Epoch 436/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1199 - accuracy: 0.9854\n",
            "Epoch 437/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0954 - accuracy: 0.9888\n",
            "Epoch 438/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1016 - accuracy: 0.9876\n",
            "Epoch 439/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1254 - accuracy: 0.9847\n",
            "Epoch 440/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1001 - accuracy: 0.9885\n",
            "Epoch 441/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0838 - accuracy: 0.9885\n",
            "Epoch 442/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0940 - accuracy: 0.9899\n",
            "Epoch 443/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0988 - accuracy: 0.9876\n",
            "Epoch 444/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1076 - accuracy: 0.9874\n",
            "Epoch 445/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1392 - accuracy: 0.9861\n",
            "Epoch 446/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1287 - accuracy: 0.9858\n",
            "Epoch 447/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1452 - accuracy: 0.9811\n",
            "Epoch 448/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1343 - accuracy: 0.9856\n",
            "Epoch 449/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1242 - accuracy: 0.9854\n",
            "Epoch 450/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1379 - accuracy: 0.9836\n",
            "Epoch 451/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1121 - accuracy: 0.9861\n",
            "Epoch 452/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1284 - accuracy: 0.9834\n",
            "Epoch 453/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1147 - accuracy: 0.9876\n",
            "Epoch 454/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1183 - accuracy: 0.9879\n",
            "Epoch 455/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1138 - accuracy: 0.9854\n",
            "Epoch 456/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1051 - accuracy: 0.9876\n",
            "Epoch 457/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1166 - accuracy: 0.9834\n",
            "Epoch 458/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1063 - accuracy: 0.9876\n",
            "Epoch 459/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1015 - accuracy: 0.9861\n",
            "Epoch 460/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0923 - accuracy: 0.9874\n",
            "Epoch 461/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1271 - accuracy: 0.9885\n",
            "Epoch 462/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1086 - accuracy: 0.9865\n",
            "Epoch 463/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1104 - accuracy: 0.9888\n",
            "Epoch 464/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1095 - accuracy: 0.9888\n",
            "Epoch 465/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1178 - accuracy: 0.9858\n",
            "Epoch 466/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1350 - accuracy: 0.9836\n",
            "Epoch 467/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1307 - accuracy: 0.9874\n",
            "Epoch 468/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1533 - accuracy: 0.9840\n",
            "Epoch 469/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0970 - accuracy: 0.9858\n",
            "Epoch 470/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1188 - accuracy: 0.9867\n",
            "Epoch 471/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1189 - accuracy: 0.9863\n",
            "Epoch 472/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1033 - accuracy: 0.9881\n",
            "Epoch 473/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1031 - accuracy: 0.9883\n",
            "Epoch 474/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1201 - accuracy: 0.9872\n",
            "Epoch 475/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1198 - accuracy: 0.9849\n",
            "Epoch 476/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1211 - accuracy: 0.9870\n",
            "Epoch 477/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9840\n",
            "Epoch 478/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1025 - accuracy: 0.9840\n",
            "Epoch 479/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1219 - accuracy: 0.9870\n",
            "Epoch 480/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9870\n",
            "Epoch 481/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1371 - accuracy: 0.9813\n",
            "Epoch 482/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1620 - accuracy: 0.9858\n",
            "Epoch 483/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1143 - accuracy: 0.9892\n",
            "Epoch 484/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1140 - accuracy: 0.9836\n",
            "Epoch 485/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1106 - accuracy: 0.9863\n",
            "Epoch 486/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1123 - accuracy: 0.9852\n",
            "Epoch 487/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.0987 - accuracy: 0.9892\n",
            "Epoch 488/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1081 - accuracy: 0.9867\n",
            "Epoch 489/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1166 - accuracy: 0.9881\n",
            "Epoch 490/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1233 - accuracy: 0.9854\n",
            "Epoch 491/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.0982 - accuracy: 0.9883\n",
            "Epoch 492/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1147 - accuracy: 0.9892\n",
            "Epoch 493/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1175 - accuracy: 0.9888\n",
            "Epoch 494/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1065 - accuracy: 0.9870\n",
            "Epoch 495/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1201 - accuracy: 0.9879\n",
            "Epoch 496/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1163 - accuracy: 0.9840\n",
            "Epoch 497/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1323 - accuracy: 0.9856\n",
            "Epoch 498/500\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.1085 - accuracy: 0.9863\n",
            "Epoch 499/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1324 - accuracy: 0.9838\n",
            "Epoch 500/500\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.1203 - accuracy: 0.9847\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22222222222222224"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int\")\n",
        "values, counts = np.unique(y_pred, return_counts=True)\n",
        "print(values, counts)\n",
        "f1_score(y_test, y_pred, average='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSoMwGfGdKDo",
        "outputId": "f7762ff0-e255-4afe-96c8-2fff3c6035d7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1] [1090   22]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22222222222222224"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGgIw7drkPqf",
        "outputId": "b0552b3c-19ce-4fe6-dbbf-cca935b883a4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1056,   15],\n",
              "       [  34,    7]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.Sequence()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UovgzM4-L1t0",
        "outputId": "ff702491-366c-41b9-8b16-db3528afa162"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.utils.data_utils.Sequence at 0x7fec46e94450>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # from imblearn.keras import BalancedBatchGenerator\n",
        "\n",
        "\n",
        "# # @ timeit\n",
        "# def fit_predict_balanced_model(X_train, y_train, X_test, y_test):\n",
        "#   model = make_model(X_train.shape[1])\n",
        "#   training_generator, steps_per_epoch = balanced_batch_generator(X_train, y_train, batch_size=128, random_state=42)\n",
        "#   callback_history = model.fit(training_generator, steps_per_epoch=steps_per_epoch, epochs=5, verbose=1)\n",
        "#   y_pred = (model.predict(X_test) > 0.5).astype('int')\n",
        "#   return f1_score(y_test, y_pred) #roc_auc_score(y_test, y_pred),"
      ],
      "metadata": {
        "id": "UEl_Lqcw3DUl"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_model(X_train.shape[1])\n",
        "training_generator, steps_per_epoch = balanced_batch_generator(X_train, y_train, batch_size=128, random_state=42)\n",
        "model.fit(training_generator, steps_per_epoch=steps_per_epoch, epochs=5, verbose=1)\n",
        "y_pred = (model.predict(X_test) > 0.5).astype('int')\n",
        "f1_score(y_test, y_pred) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "CsrVXoj58O-D",
        "outputId": "2b5097ce-17dd-44b3-c59a-37bace14da96"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f1bad17d0cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbalanced_batch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_get_dynamic_shape\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    819\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;31m# Unknown number of dimensions, `as_list` cannot be called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import keras.utils.all_utils.Sequence\n",
        "# from imblearn.keras import BalancedBatchGenerator"
      ],
      "metadata": {
        "id": "sPcOQVEwm_s3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "\n",
        "cv_results_imbalanced = []\n",
        "# cv_time_imbalanced = []\n",
        "cv_results_balanced = []\n",
        "# cv_time_balanced = []\n",
        "for train_idx, valid_idx in skf.split(X_train, y_train):\n",
        "  X_local_train = X_train[train_idx]\n",
        "  y_local_train = y_train[train_idx]\n",
        "  X_local_test = X_train[valid_idx]\n",
        "  y_local_test = y_train[valid_idx]\n",
        "  f1 = fit_predict_imbalanced_model( #roc_auc, elapsed_time, \n",
        "      X_local_train, y_local_train, X_local_test, y_local_test\n",
        "  )\n",
        "  # cv_time_imbalanced.append(elapsed_time)\n",
        "  # cv_results_imbalanced.append(roc_auc)\n",
        "  cv_results_imbalanced.append(f1)\n",
        "\n",
        "  # f1 = fit_predict_balanced_model( # roc_auc, elapsed_time, \n",
        "  #     X_local_train, y_local_train, X_local_test, y_local_test\n",
        "  # )\n",
        "  # cv_time_balanced.append(elapsed_time)\n",
        "  # cv_results_balanced.append(roc_auc)\n",
        "  # cv_results_imbalanced.append(f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "egaTnE5h3dbx",
        "outputId": "b36c351a-5ff0-4ed0-af04-33cb863ba553"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7dc39b51004f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mX_local_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0my_local_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mX_local_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0my_local_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([ 444,  445,  446,  447,  448,  449,  450,  451,  452,  453,\\n            ...\\n            4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444],\\n           dtype='int64', length=4000)] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "\n",
        "from imblearn.datasets import make_imbalance\n",
        "class_dict = dict()\n",
        "class_dict[0] = 30; class_dict[1] = 50; class_dict[2] = 40\n",
        "X, y = make_imbalance(iris.data, iris.target, sampling_strategy=class_dict)\n",
        "\n",
        "import tensorflow\n",
        "y = tensorflow.keras.utils.to_categorical(y, 3)\n",
        "model = tensorflow.keras.models.Sequential()\n",
        "model.add(\n",
        "    tensorflow.keras.layers.Dense(\n",
        "        y.shape[1], input_dim=X.shape[1], activation='softmax'\n",
        "    )\n",
        ")\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "from imblearn.keras import BalancedBatchGenerator\n",
        "from imblearn.under_sampling import NearMiss\n",
        "training_generator = BalancedBatchGenerator(\n",
        "    X, y, sampler=NearMiss(), batch_size=10, random_state=42)\n",
        "callback_history = model.fit(training_generator, epochs=10, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "k95WM_nS6q-b",
        "outputId": "17dc9b10-97f2-4143-bf7e-98912c6727c3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1338cfa7b49f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBalancedBatchGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNearMiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m training_generator = BalancedBatchGenerator(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m in keras.\"\"\"\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBalancedBatchGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbalanced_batch_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/keras/_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mParentClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHAS_KERAS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/keras/_generator.py\u001b[0m in \u001b[0;36mimport_keras\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mParentClassKeras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_keras_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_from_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mParentClassTensorflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_keras_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_from_tensforflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mhas_keras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhas_keras_k\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhas_keras_tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/keras/_generator.py\u001b[0m in \u001b[0;36mimport_from_keras\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.utils' has no attribute 'Sequence'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model_v3"
      ],
      "metadata": {
        "id": "4NFq1LGkNzmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[y_train['Active']==1].count(), y_train[y_train['Active']==0].count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U26VZjSSOLi9",
        "outputId": "e656ccd0-e93a-42f4-a4b7-dcebde7becf8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Active    165\n",
              " dtype: int64, Active    4280\n",
              " dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = np.bincount(y_train.values[:, 0])\n",
        "counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQzTT7VhPfj4",
        "outputId": "36e4f9a1-7e59-4110-d1c2-ab5a86ea208f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4280,  165])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = np.bincount(y_train.values[:, 0])\n",
        "print(\n",
        "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
        "        counts[1], 100 * float(counts[1]) / len(y_train.values)\n",
        "    )\n",
        ")\n",
        "\n",
        "weight_for_0 = 1.0 / counts[0]\n",
        "weight_for_1 = 1.0 / counts[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7LyUD65N45t",
        "outputId": "dbf2af7b-2c05-474e-d99d-c251e5ca1185"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive samples in training data: 165 (3.71% of total)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Dense(\n",
        "            256, activation=\"relu\", input_shape=(X_train.shape[-1],)\n",
        "        ),\n",
        "        keras.layers.Dense(256, activation=\"relu\"),\n",
        "        keras.layers.Dropout(0.3),\n",
        "        keras.layers.Dense(256, activation=\"relu\"),\n",
        "        keras.layers.Dropout(0.3),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80w4VxNdN5Da",
        "outputId": "ac70b1d1-6a2d-447e-9720-eeb4b8e5d04a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               26880     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 158,721\n",
            "Trainable params: 158,721\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [\n",
        "    keras.metrics.FalseNegatives(name=\"fn\"),\n",
        "    keras.metrics.FalsePositives(name=\"fp\"),\n",
        "    keras.metrics.TrueNegatives(name=\"tn\"),\n",
        "    keras.metrics.TruePositives(name=\"tp\"),\n",
        "    keras.metrics.Precision(name=\"precision\"),\n",
        "    keras.metrics.Recall(name=\"recall\"),\n",
        "]\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrics\n",
        ")\n",
        "\n",
        "# callbacks = [keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.h5\")]\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=2048,\n",
        "    epochs=30,\n",
        "    verbose=2,\n",
        "    # callbacks=callbacks,\n",
        "    # validation_data=(val_features, val_targets),\n",
        "    class_weight=class_weight,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSK2BSVpN5ZJ",
        "outputId": "971560de-2af4-472b-8ac7-487a88f76a44"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "3/3 - 2s - loss: 3.6141e-04 - fn: 90.0000 - fp: 1755.0000 - tn: 2525.0000 - tp: 75.0000 - precision: 0.0410 - recall: 0.4545 - 2s/epoch - 775ms/step\n",
            "Epoch 2/30\n",
            "3/3 - 0s - loss: 3.0357e-04 - fn: 23.0000 - fp: 2999.0000 - tn: 1281.0000 - tp: 142.0000 - precision: 0.0452 - recall: 0.8606 - 55ms/epoch - 18ms/step\n",
            "Epoch 3/30\n",
            "3/3 - 0s - loss: 3.1138e-04 - fn: 110.0000 - fp: 662.0000 - tn: 3618.0000 - tp: 55.0000 - precision: 0.0767 - recall: 0.3333 - 46ms/epoch - 15ms/step\n",
            "Epoch 4/30\n",
            "3/3 - 0s - loss: 2.6980e-04 - fn: 30.0000 - fp: 2114.0000 - tn: 2166.0000 - tp: 135.0000 - precision: 0.0600 - recall: 0.8182 - 44ms/epoch - 15ms/step\n",
            "Epoch 5/30\n",
            "3/3 - 0s - loss: 2.6278e-04 - fn: 56.0000 - fp: 1096.0000 - tn: 3184.0000 - tp: 109.0000 - precision: 0.0905 - recall: 0.6606 - 49ms/epoch - 16ms/step\n",
            "Epoch 6/30\n",
            "3/3 - 0s - loss: 2.4422e-04 - fn: 52.0000 - fp: 991.0000 - tn: 3289.0000 - tp: 113.0000 - precision: 0.1024 - recall: 0.6848 - 57ms/epoch - 19ms/step\n",
            "Epoch 7/30\n",
            "3/3 - 0s - loss: 2.4147e-04 - fn: 45.0000 - fp: 970.0000 - tn: 3310.0000 - tp: 120.0000 - precision: 0.1101 - recall: 0.7273 - 47ms/epoch - 16ms/step\n",
            "Epoch 8/30\n",
            "3/3 - 0s - loss: 2.3850e-04 - fn: 40.0000 - fp: 1256.0000 - tn: 3024.0000 - tp: 125.0000 - precision: 0.0905 - recall: 0.7576 - 47ms/epoch - 16ms/step\n",
            "Epoch 9/30\n",
            "3/3 - 0s - loss: 2.2328e-04 - fn: 47.0000 - fp: 923.0000 - tn: 3357.0000 - tp: 118.0000 - precision: 0.1134 - recall: 0.7152 - 46ms/epoch - 15ms/step\n",
            "Epoch 10/30\n",
            "3/3 - 0s - loss: 2.3303e-04 - fn: 37.0000 - fp: 1291.0000 - tn: 2989.0000 - tp: 128.0000 - precision: 0.0902 - recall: 0.7758 - 48ms/epoch - 16ms/step\n",
            "Epoch 11/30\n",
            "3/3 - 0s - loss: 2.1611e-04 - fn: 30.0000 - fp: 1093.0000 - tn: 3187.0000 - tp: 135.0000 - precision: 0.1099 - recall: 0.8182 - 46ms/epoch - 15ms/step\n",
            "Epoch 12/30\n",
            "3/3 - 0s - loss: 2.1840e-04 - fn: 43.0000 - fp: 798.0000 - tn: 3482.0000 - tp: 122.0000 - precision: 0.1326 - recall: 0.7394 - 50ms/epoch - 17ms/step\n",
            "Epoch 13/30\n",
            "3/3 - 0s - loss: 1.9554e-04 - fn: 29.0000 - fp: 781.0000 - tn: 3499.0000 - tp: 136.0000 - precision: 0.1483 - recall: 0.8242 - 51ms/epoch - 17ms/step\n",
            "Epoch 14/30\n",
            "3/3 - 0s - loss: 1.9866e-04 - fn: 38.0000 - fp: 744.0000 - tn: 3536.0000 - tp: 127.0000 - precision: 0.1458 - recall: 0.7697 - 46ms/epoch - 15ms/step\n",
            "Epoch 15/30\n",
            "3/3 - 0s - loss: 1.8252e-04 - fn: 36.0000 - fp: 673.0000 - tn: 3607.0000 - tp: 129.0000 - precision: 0.1608 - recall: 0.7818 - 48ms/epoch - 16ms/step\n",
            "Epoch 16/30\n",
            "3/3 - 0s - loss: 1.8840e-04 - fn: 23.0000 - fp: 978.0000 - tn: 3302.0000 - tp: 142.0000 - precision: 0.1268 - recall: 0.8606 - 48ms/epoch - 16ms/step\n",
            "Epoch 17/30\n",
            "3/3 - 0s - loss: 2.0013e-04 - fn: 30.0000 - fp: 917.0000 - tn: 3363.0000 - tp: 135.0000 - precision: 0.1283 - recall: 0.8182 - 54ms/epoch - 18ms/step\n",
            "Epoch 18/30\n",
            "3/3 - 0s - loss: 1.8660e-04 - fn: 37.0000 - fp: 636.0000 - tn: 3644.0000 - tp: 128.0000 - precision: 0.1675 - recall: 0.7758 - 47ms/epoch - 16ms/step\n",
            "Epoch 19/30\n",
            "3/3 - 0s - loss: 2.1598e-04 - fn: 22.0000 - fp: 1378.0000 - tn: 2902.0000 - tp: 143.0000 - precision: 0.0940 - recall: 0.8667 - 48ms/epoch - 16ms/step\n",
            "Epoch 20/30\n",
            "3/3 - 0s - loss: 1.9992e-04 - fn: 47.0000 - fp: 486.0000 - tn: 3794.0000 - tp: 118.0000 - precision: 0.1954 - recall: 0.7152 - 47ms/epoch - 16ms/step\n",
            "Epoch 21/30\n",
            "3/3 - 0s - loss: 1.7422e-04 - fn: 15.0000 - fp: 1044.0000 - tn: 3236.0000 - tp: 150.0000 - precision: 0.1256 - recall: 0.9091 - 52ms/epoch - 17ms/step\n",
            "Epoch 22/30\n",
            "3/3 - 0s - loss: 1.6174e-04 - fn: 20.0000 - fp: 778.0000 - tn: 3502.0000 - tp: 145.0000 - precision: 0.1571 - recall: 0.8788 - 49ms/epoch - 16ms/step\n",
            "Epoch 23/30\n",
            "3/3 - 0s - loss: 1.6385e-04 - fn: 23.0000 - fp: 605.0000 - tn: 3675.0000 - tp: 142.0000 - precision: 0.1901 - recall: 0.8606 - 50ms/epoch - 17ms/step\n",
            "Epoch 24/30\n",
            "3/3 - 0s - loss: 1.4233e-04 - fn: 27.0000 - fp: 401.0000 - tn: 3879.0000 - tp: 138.0000 - precision: 0.2560 - recall: 0.8364 - 51ms/epoch - 17ms/step\n",
            "Epoch 25/30\n",
            "3/3 - 0s - loss: 1.4239e-04 - fn: 13.0000 - fp: 693.0000 - tn: 3587.0000 - tp: 152.0000 - precision: 0.1799 - recall: 0.9212 - 48ms/epoch - 16ms/step\n",
            "Epoch 26/30\n",
            "3/3 - 0s - loss: 1.2571e-04 - fn: 20.0000 - fp: 480.0000 - tn: 3800.0000 - tp: 145.0000 - precision: 0.2320 - recall: 0.8788 - 47ms/epoch - 16ms/step\n",
            "Epoch 27/30\n",
            "3/3 - 0s - loss: 1.3450e-04 - fn: 18.0000 - fp: 537.0000 - tn: 3743.0000 - tp: 147.0000 - precision: 0.2149 - recall: 0.8909 - 48ms/epoch - 16ms/step\n",
            "Epoch 28/30\n",
            "3/3 - 0s - loss: 1.3863e-04 - fn: 17.0000 - fp: 617.0000 - tn: 3663.0000 - tp: 148.0000 - precision: 0.1935 - recall: 0.8970 - 50ms/epoch - 17ms/step\n",
            "Epoch 29/30\n",
            "3/3 - 0s - loss: 1.5144e-04 - fn: 16.0000 - fp: 520.0000 - tn: 3760.0000 - tp: 149.0000 - precision: 0.2227 - recall: 0.9030 - 48ms/epoch - 16ms/step\n",
            "Epoch 30/30\n",
            "3/3 - 0s - loss: 1.3955e-04 - fn: 19.0000 - fp: 620.0000 - tn: 3660.0000 - tp: 146.0000 - precision: 0.1906 - recall: 0.8848 - 47ms/epoch - 16ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f897028ff50>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = (model.predict(X_test) > 0.5).astype(\"int\")\n",
        "values, counts = np.unique(pred, return_counts=True)\n",
        "print(values, counts)\n",
        "f1_score(y_test, pred, average='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTpuJd2sN5bl",
        "outputId": "ec11c485-ed45-4d55-87e7-879fec47357a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1] [912 200]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14937759336099585"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7vBM0kZgN5gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_pFik_tpN5i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qt2guu9DN5ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Test data"
      ],
      "metadata": {
        "id": "cQm0cPg6a3Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/Hackathon_Innopolis/test.csv', usecols=['Smiles'])\n",
        "new_test_dataset = pd.read_csv('/content/drive/MyDrive/Hackathon_Innopolis/new_test_dataset.csv')"
      ],
      "metadata": {
        "id": "eBC4pQz6SpPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to_csv Submission.csv\n",
        "X_test_dataset = StandardScaler().fit_transform(new_test_dataset)\n",
        "test_prediction = (model.predict(X_test_dataset) > 0.5).astype(\"int\")\n",
        "print(test_prediction)\n",
        "\n",
        "values, counts = np.unique(test_prediction, return_counts=True)\n",
        "print(values, counts)\n",
        "\n",
        "# df_sub = pd.read_csv('/content/drive/MyDrive/Hackathon_Innopolis/test.csv', usecols=['Smiles'])\n",
        "df_test['Active'] = test_prediction\n",
        "print(df_test['Active'].value_counts())\n",
        "\n",
        "df_test.to_csv('submission.csv', index=False)\n",
        "df = pd.read_csv('/content/submission.csv')\n",
        "print(df['Active'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqTD0fT6OUzL",
        "outputId": "460e8abf-e100-4384-f700-0050691c206b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [1]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n",
            "[0 1] [1593   21]\n",
            "0    1593\n",
            "1      21\n",
            "Name: Active, dtype: int64\n",
            "0    1593\n",
            "1      21\n",
            "Name: Active, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3cwji9TTYvI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}